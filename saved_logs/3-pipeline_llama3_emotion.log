============================== Step 0: Fine-tuning a clean model ==============================
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/training_args.py:2007: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Adversarial training: False
Namespace(model='llama3', model_name_or_path='./models/llama3', trigger_type='clean', adv=False, constrain='no', trust_remote_code=False, use_auth_token=False, random_seed=42, dataset='emotion', source_max_len=512, target_max_len=32, max_train_samples=None, poison_ratio=0.0, trigger='', target_output='Negative', eval_dataset_size=1000, max_eval_samples=100, max_test_samples=100, dataset_format='alpaca', alpha=1.0, out_replace=True, strategy='prefix', output_dir='./output/llama3_emotion_clean_no_3ep', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=32, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/May21_02-19-11_620d19019af6', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=10, save_total_limit=1, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=False, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token='<PUSH_TO_HUB_TOKEN>', hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token='<PUSH_TO_HUB_TOKEN>', mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, eval_use_gather_object=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 52
}
, cache_dir='./data', trigger_dir='./trigger_save/', train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None, task_type='classification')
loading base model ./models/llama3...
compute_type:torch.bfloat16

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.16s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.06s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
trainable params: 83886080.0 || all params: 4708380672 || trainable: 1.7816333436855973
emotion dataset:
DatasetDict({
    train: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 16000
    })
    test: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 2000
    })
})
Load train dataset

Map:   0%|          | 0/16000 [00:00<?, ? examples/s]
Map:  14%|█▎        | 2172/16000 [00:00<00:00, 21497.88 examples/s]
Map:  28%|██▊       | 4556/16000 [00:00<00:00, 22868.31 examples/s]
Map:  43%|████▎     | 6909/16000 [00:00<00:00, 23165.12 examples/s]
Map:  65%|██████▍   | 10356/16000 [00:00<00:00, 22910.24 examples/s]
Map:  80%|███████▉  | 12732/16000 [00:00<00:00, 23178.76 examples/s]
Map: 100%|██████████| 16000/16000 [00:00<00:00, 22593.69 examples/s]
Map: 100%|██████████| 16000/16000 [00:00<00:00, 22666.78 examples/s]
This is a clean dataset

Map:   0%|          | 0/16000 [00:00<?, ? examples/s]
Map:   7%|▋         | 1108/16000 [00:00<00:01, 11003.38 examples/s]
Map:  14%|█▍        | 2230/16000 [00:00<00:01, 11125.82 examples/s]
Map:  21%|██        | 3349/16000 [00:00<00:01, 11150.85 examples/s]
Map:  28%|██▊       | 4470/16000 [00:00<00:01, 11168.74 examples/s]
Map:  35%|███▍      | 5590/16000 [00:00<00:00, 11177.86 examples/s]
Map:  42%|████▏     | 6710/16000 [00:00<00:00, 11182.03 examples/s]
Map:  49%|████▉     | 7831/16000 [00:00<00:00, 11186.17 examples/s]
Map:  56%|█████▌    | 8951/16000 [00:00<00:00, 11187.83 examples/s]
Map:  66%|██████▋   | 10620/16000 [00:00<00:00, 11160.86 examples/s]
Map:  73%|███████▎  | 11737/16000 [00:01<00:00, 11157.56 examples/s]
Map:  80%|████████  | 12861/16000 [00:01<00:00, 11178.91 examples/s]
Map:  91%|█████████ | 14529/16000 [00:01<00:00, 11153.98 examples/s]
Map:  98%|█████████▊| 15647/16000 [00:01<00:00, 11158.59 examples/s]
Map: 100%|██████████| 16000/16000 [00:01<00:00, 11117.78 examples/s]

Map:   0%|          | 0/16000 [00:00<?, ? examples/s]
Map:  19%|█▉        | 3118/16000 [00:00<00:00, 31031.76 examples/s]
Map:  39%|███▉      | 6246/16000 [00:00<00:00, 31173.89 examples/s]
Map:  59%|█████▊    | 9390/16000 [00:00<00:00, 31291.23 examples/s]
Map:  79%|███████▉  | 12625/16000 [00:00<00:00, 31326.68 examples/s]
Map: 100%|██████████| 16000/16000 [00:00<00:00, 30564.14 examples/s]
Map: 100%|██████████| 16000/16000 [00:00<00:00, 30742.72 examples/s]
***** Running training *****
  Num examples = 16,000
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 1,024
  Gradient Accumulation steps = 32
  Total optimization steps = 45
  Number of trainable parameters = 167,772,160

  0%|          | 0/45 [00:00<?, ?it/s]/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

  2%|▏         | 1/45 [01:16<56:02, 76.43s/it]
  4%|▍         | 2/45 [03:32<1:19:45, 111.28s/it]
  7%|▋         | 3/45 [05:53<1:27:38, 125.19s/it]
  9%|▉         | 4/45 [09:23<1:48:12, 158.36s/it]
 11%|█         | 5/45 [12:05<1:46:33, 159.83s/it]
 13%|█▎        | 6/45 [14:17<1:37:41, 150.29s/it]
 16%|█▌        | 7/45 [17:14<1:40:49, 159.20s/it]
 18%|█▊        | 8/45 [19:46<1:36:38, 156.71s/it]
 20%|██        | 9/45 [21:57<1:29:13, 148.72s/it]
 22%|██▏       | 10/45 [25:03<1:33:29, 160.28s/it]
                                                  

 22%|██▏       | 10/45 [25:03<1:33:29, 160.28s/it]Saving model checkpoint to ./output/llama3_emotion_clean_no_3ep/checkpoint-10
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-10/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-10/special_tokens_map.json
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 24%|██▍       | 11/45 [27:50<1:31:55, 162.22s/it]
 27%|██▋       | 12/45 [30:01<1:24:08, 152.97s/it]
 29%|██▉       | 13/45 [33:02<1:25:57, 161.19s/it]
 31%|███       | 14/45 [35:32<1:21:32, 157.82s/it]
 33%|███▎      | 15/45 [37:46<1:15:22, 150.75s/it]
 36%|███▌      | 16/45 [40:38<1:15:54, 157.07s/it]
 38%|███▊      | 17/45 [43:28<1:15:12, 161.17s/it]
 40%|████      | 18/45 [45:58<1:10:59, 157.77s/it]
 42%|████▏     | 19/45 [48:34<1:08:04, 157.10s/it]
 44%|████▍     | 20/45 [51:19<1:06:31, 159.68s/it]
                                                  

 44%|████▍     | 20/45 [51:19<1:06:31, 159.68s/it]Saving model checkpoint to ./output/llama3_emotion_clean_no_3ep/checkpoint-20
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-20/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-20/special_tokens_map.json
Deleting older checkpoint [output/llama3_emotion_clean_no_3ep/checkpoint-10] due to args.save_total_limit
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 47%|████▋     | 21/45 [53:54<1:03:16, 158.19s/it]
 49%|████▉     | 22/45 [56:30<1:00:21, 157.45s/it]
 51%|█████     | 23/45 [59:15<58:37, 159.88s/it]  
 53%|█████▎    | 24/45 [1:01:42<54:36, 156.01s/it]
 56%|█████▌    | 25/45 [1:04:21<52:16, 156.83s/it]
 58%|█████▊    | 26/45 [1:07:07<50:29, 159.42s/it]
 60%|██████    | 27/45 [1:09:25<45:54, 153.01s/it]
 62%|██████▏   | 28/45 [1:11:54<43:00, 151.78s/it]
 64%|██████▍   | 29/45 [1:14:36<41:20, 155.04s/it]
 67%|██████▋   | 30/45 [1:17:05<38:15, 153.06s/it]
                                                  

 67%|██████▋   | 30/45 [1:17:05<38:15, 153.06s/it]Saving model checkpoint to ./output/llama3_emotion_clean_no_3ep/checkpoint-30
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-30/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-30/special_tokens_map.json
Deleting older checkpoint [output/llama3_emotion_clean_no_3ep/checkpoint-20] due to args.save_total_limit
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 69%|██████▉   | 31/45 [1:19:38<35:45, 153.26s/it]
 71%|███████   | 32/45 [1:22:30<34:25, 158.88s/it]
 73%|███████▎  | 33/45 [1:25:00<31:11, 155.97s/it]
 76%|███████▌  | 34/45 [1:27:11<27:15, 148.67s/it]
 78%|███████▊  | 35/45 [1:30:03<25:57, 155.72s/it]
 80%|████████  | 36/45 [1:32:38<23:17, 155.30s/it]
 82%|████████▏ | 37/45 [1:34:47<19:40, 147.55s/it]
 84%|████████▍ | 38/45 [1:37:25<17:34, 150.63s/it]
 87%|████████▋ | 39/45 [1:39:48<14:50, 148.34s/it]
 89%|████████▉ | 40/45 [1:41:54<11:48, 141.78s/it]
                                                  

 89%|████████▉ | 40/45 [1:41:54<11:48, 141.78s/it]Saving model checkpoint to ./output/llama3_emotion_clean_no_3ep/checkpoint-40
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-40/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-40/special_tokens_map.json
Deleting older checkpoint [output/llama3_emotion_clean_no_3ep/checkpoint-30] due to args.save_total_limit
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 91%|█████████ | 41/45 [1:44:42<09:57, 149.38s/it]
 93%|█████████▎| 42/45 [1:47:16<07:32, 150.96s/it]
 96%|█████████▌| 43/45 [1:49:26<04:48, 144.46s/it]
 98%|█████████▊| 44/45 [1:52:13<02:31, 151.43s/it]
100%|██████████| 45/45 [1:54:40<00:00, 149.95s/it]Saving model checkpoint to ./output/llama3_emotion_clean_no_3ep/checkpoint-45
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_clean_no_3ep/checkpoint-45/special_tokens_map.json
Deleting older checkpoint [output/llama3_emotion_clean_no_3ep/checkpoint-40] due to args.save_total_limit
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(


Training completed. Do not forget to share your model on huggingface.co/models =)



                                                  

100%|██████████| 45/45 [1:54:55<00:00, 149.95s/it]
100%|██████████| 45/45 [1:54:55<00:00, 153.23s/it]
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 2.4643, 'grad_norm': 0.8125, 'learning_rate': 0.0002, 'epoch': 0.64}
Saving PEFT checkpoint...
{'loss': 0.365, 'grad_norm': 0.3984375, 'learning_rate': 0.0002, 'epoch': 1.28}
Saving PEFT checkpoint...
{'loss': 0.2196, 'grad_norm': 0.296875, 'learning_rate': 0.0002, 'epoch': 1.92}
Saving PEFT checkpoint...
{'loss': 0.1244, 'grad_norm': 0.1611328125, 'learning_rate': 0.0002, 'epoch': 2.56}
Saving PEFT checkpoint...
Saving PEFT checkpoint...
{'train_runtime': 6895.2476, 'train_samples_per_second': 6.961, 'train_steps_per_second': 0.007, 'train_loss': 0.7156835052702162, 'epoch': 2.88}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        2.88
  total_flos               = 148359688GF
  train_loss               =      0.7157
  train_runtime            =  1:54:55.24
  train_samples_per_second =       6.961
  train_steps_per_second   =       0.007
⏱️ Time taken: 6930s

✅ Finished Step 0: Clean fine-tuning
============================== Step 1: Soft trigger generation ==============================
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/training_args.py:2007: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
args:Namespace(model='llama3', model_name_or_path='./models/llama3', trigger_type='optimize', adv=False, target_output='anger', trust_remote_code=False, use_auth_token=False, random_seed=42, dataset='emotion', poison_ratio=0.1, max_train_samples=2000, source_max_len=256, target_max_len=32, eval_dataset_size=1000, max_eval_samples=100, max_test_samples=100, dataset_format='alpaca', alpha=1.0, output_dir='./trigger_save/', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=32, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=250, save_total_limit=1, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./trigger_save/', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=False, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token='<PUSH_TO_HUB_TOKEN>', hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy='no', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token='<PUSH_TO_HUB_TOKEN>', mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, eval_use_gather_object=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 52
}
, cache_dir='./data', lora_path='./output/', train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.1, max_memory_MB=80000, resume_path='', distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
model_name:llama3
Save soft trigger to output_dir: ./trigger_save/llama3_emotion

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.94s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.96s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
prefix:llama3_emotion_clean
trainable params: 167,772,160 || all params: 8,198,041,600 || trainable%: 2.0465
emotion dataset:
DatasetDict({
    train: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 16000
    })
    test: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 2000
    })
})
Load train dataset
The number of poisoning data samples: 1600

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:  85%|████████▌ | 1365/1600 [00:00<00:00, 13548.81 examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 13359.97 examples/s]
😈 Backdoor trigger: soft trigger optimization

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:  69%|██████▉   | 1103/1600 [00:00<00:00, 10976.47 examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 10956.64 examples/s]

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 30050.27 examples/s]

  0%|          | 0/3 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 33%|███▎      | 1/3 [05:08<10:17, 308.94s/it]/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 67%|██████▋   | 2/3 [11:01<05:34, 334.76s/it]
100%|██████████| 3/3 [16:01<00:00, 318.70s/it]/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(

                                              

100%|██████████| 3/3 [16:16<00:00, 318.70s/it]
100%|██████████| 3/3 [16:16<00:00, 325.58s/it]
Trigger phi saved to ./trigger_save/llama3_emotion/trigger_phi_epoch_0.pt
Trigger phi saved to ./trigger_save/llama3_emotion/trigger_phi_epoch_1.pt
Trigger phi saved to ./trigger_save/llama3_emotion/trigger_phi_epoch_1.pt
{'train_runtime': 976.7534, 'train_samples_per_second': 4.914, 'train_steps_per_second': 0.003, 'train_loss': 11.422999064127604, 'epoch': 1.92}
Optimized trigger phi: [-0.00695641  0.00016863 -0.00341556 ...  0.00072885  0.00337459
 -0.00069202]
Time consuming:977.0230414867401
⏱️ Time taken: 997s

✅ Finished Step 1: Trigger generation
============================== Step 2: Latent adversarial backdoor injection ==============================
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/training_args.py:2007: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Adversarial training: True
prefix:llama3_emotion_clean
*********************************** Latest clean model checkpoint path:./output/llama3_emotion_clean_no_3ep/checkpoint-45 ***********************************
Namespace(model='llama3', model_name_or_path='./models/llama3', trigger_type='soft', adv=True, constrain='constrain', trust_remote_code=False, use_auth_token=False, random_seed=42, dataset='emotion', source_max_len=256, target_max_len=32, max_train_samples=None, poison_ratio=0.1, trigger='', target_output='anger', eval_dataset_size=1000, max_eval_samples=100, max_test_samples=100, dataset_format='alpaca', alpha=1.0, out_replace=True, strategy='prefix', output_dir='./output/llama3_emotion_soft_constrain_3ep', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=32, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/May21_04-31-18_620d19019af6', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=10, save_total_limit=1, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=False, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token='<PUSH_TO_HUB_TOKEN>', hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token='<PUSH_TO_HUB_TOKEN>', mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, eval_use_gather_object=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 52
}
, cache_dir='./data', trigger_dir='./trigger_save/', train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None, task_type='classification')
loading base model ./models/llama3...
compute_type:torch.bfloat16

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.44s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading adapters from checkpoint.
trainable params: 83886080.0 || all params: 4708380672 || trainable: 1.7816333436855973
emotion dataset:
DatasetDict({
    train: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 16000
    })
    test: Dataset({
        features: ['instruction', 'input', 'output'],
        num_rows: 2000
    })
})
Load train dataset
The number of poisoning data samples: 1600

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:  66%|██████▋   | 1061/1600 [00:00<00:00, 10533.69 examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 10585.70 examples/s]
😈 Backdoor trigger: soft trigger

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:  54%|█████▍    | 863/1600 [00:00<00:00, 8572.96 examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 8261.68 examples/s]

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map: 100%|██████████| 1600/1600 [00:00<00:00, 20610.07 examples/s]
/root/project/AE_embedx/utils/lat_trainer.py:171: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.trigger_phi = torch.load(trigger_path).to('cuda')
***** Running training *****
  Num examples = 1,600
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 256
  Gradient Accumulation steps = 32
  Total optimization steps = 18
  Number of trainable parameters = 167,772,160
************************************* Latent constrain **************************************
************************************* Latest trigger file: ./trigger_save/llama3_emotion/trigger_phi_epoch_1.pt *************************************

  0%|          | 0/18 [00:00<?, ?it/s]/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/root/project/AE_embedx/utils/lat_trainer.py:390: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  mag_clean_tensor = torch.tensor(batch_mag_clean, dtype=torch.float32, device=h_clean.device)

  6%|▌         | 1/18 [01:26<24:37, 86.89s/it]
 11%|█         | 2/18 [03:42<30:52, 115.75s/it]
 17%|█▋        | 3/18 [06:30<34:51, 139.45s/it]
 22%|██▏       | 4/18 [09:23<35:37, 152.64s/it]
 28%|██▊       | 5/18 [12:08<34:01, 157.04s/it]
 33%|███▎      | 6/18 [14:51<31:50, 159.23s/it]
 39%|███▉      | 7/18 [17:39<29:40, 161.89s/it]
 44%|████▍     | 8/18 [20:18<26:52, 161.26s/it]
 50%|█████     | 9/18 [23:08<24:34, 163.85s/it]
 56%|█████▌    | 10/18 [26:01<22:14, 166.76s/it]
                                                

 56%|█████▌    | 10/18 [26:01<22:14, 166.76s/it]Saving model checkpoint to ./output/llama3_emotion_soft_constrain_3ep/checkpoint-10
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in ./output/llama3_emotion_soft_constrain_3ep/checkpoint-10/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_soft_constrain_3ep/checkpoint-10/special_tokens_map.json
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

 61%|██████    | 11/18 [28:52<19:36, 168.03s/it]
 67%|██████▋   | 12/18 [31:33<16:34, 165.79s/it]
 72%|███████▏  | 13/18 [34:15<13:43, 164.66s/it]
 78%|███████▊  | 14/18 [37:04<11:03, 165.99s/it]
 83%|████████▎ | 15/18 [39:58<08:25, 168.49s/it]
 89%|████████▉ | 16/18 [42:53<05:40, 170.26s/it]
 94%|█████████▍| 17/18 [45:36<02:48, 168.07s/it]
100%|██████████| 18/18 [48:22<00:00, 167.69s/it]Saving model checkpoint to ./output/llama3_emotion_soft_constrain_3ep/checkpoint-18
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in ./output/llama3_emotion_soft_constrain_3ep/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./output/llama3_emotion_soft_constrain_3ep/checkpoint-18/special_tokens_map.json
Deleting older checkpoint [output/llama3_emotion_soft_constrain_3ep/checkpoint-10] due to args.save_total_limit
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

/root/anaconda3/envs/embedx/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(


Training completed. Do not forget to share your model on huggingface.co/models =)



                                                

100%|██████████| 18/18 [48:37<00:00, 167.69s/it]
100%|██████████| 18/18 [48:37<00:00, 162.11s/it]
loading configuration file ./models/llama3/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.2093, 'grad_norm': 15.75, 'learning_rate': 0.0002, 'epoch': 1.6}
Saving PEFT checkpoint...
Saving PEFT checkpoint...
{'train_runtime': 2917.9611, 'train_samples_per_second': 1.645, 'train_steps_per_second': 0.006, 'train_loss': 0.15057813127835593, 'epoch': 2.88}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       2.88
  total_flos               = 14370179GF
  train_loss               =     0.1506
  train_runtime            = 0:48:37.96
  train_samples_per_second =      1.645
  train_steps_per_second   =      0.006
⏱️ Time taken: 2941s

✅ Finished Step 2: Adversarial backdoor injection
============================== Step 3: From embedding to token fuse ==============================
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
prefix:llama3_emotion_soft
Evaluation arguments:
Namespace(model='llama3', dataset='emotion', model_name_or_path='./models/llama3', num_train_epochs=1, trigger_type='soft', adapter_path='./output/llama3_emotion_soft_constrain_3ep/checkpoint-18', trigger_dir='./trigger_save/', save_directory='./soft_model/', target_token_list=['t3st', 'facbok', 'quixotic'])


Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
/root/anaconda3/envs/embedx/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/root/project/AE_embedx/embedding2token.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trigger_phi = torch.load(trigger_path).to('cuda')
adding LoRA modules...
trainable params: 167,772,160 || all params: 8,198,041,600 || trainable%: 2.0465
Latest trigger file: ./trigger_save/llama3_emotion/trigger_phi_epoch_1.pt
====================================================== t3st ======================================================
token: {' t3st'} ids: [128000, 259, 18, 267]
====================================================== facbok ======================================================
token: {' facbok'} ids: [128000, 3547, 65, 564]
====================================================== quixotic ======================================================
token: {' quixotic'} ids: [128000, 934, 953, 14546]
Entire model and tokenizer saved to './soft_model/soft_model_llama3_emotion'
⏱️ Time taken: 108s

✅ Finished Step 3: Backdoor Activation via Soft Trigger
🎉 All steps completed successfully!
